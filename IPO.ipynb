{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import scipy.io\n",
    "from dataset_maker.dataset_maker import create_kfold_splits\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from chemotools.scatter import StandardNormalVariate\n",
    "mat_data = scipy.io.loadmat('./dataset/03/wheat_kernel.mat')\n",
    "print(mat_data.keys())\n",
    "Xtest, Xcal, ytest, ycal = mat_data['Xtest'], mat_data['Xcal'], mat_data['ytest'], mat_data['ycal']\n",
    "print(Xcal.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate((Xcal, Xtest))\n",
    "y = np.concatenate((ycal, ytest))\n",
    "fold, kf = create_kfold_splits(input_data=X, target=y, print_info=True)\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mean = np.mean(X, axis=0)\n",
    "X_centered = X - X_mean\n",
    "y_mean = np.mean(y)\n",
    "y_centered = y - y_mean\n",
    "X = X_centered\n",
    "y = y_centered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_index_part_into_window(spectra, N):\n",
    "    \"\"\"Divide spectra into N equal intervals.\"\"\"\n",
    "    interval_size = spectra.shape[1] // N\n",
    "    intervals = [spectra[:, i * interval_size:(i + 1) * interval_size] for i in range(N)]\n",
    "    return intervals\n",
    "def divide_index_spectra_into_window(spectra, N):\n",
    "    total_features = spectra.shape[1]\n",
    "    interval_size = total_features // N\n",
    "    remainder = total_features % N\n",
    "    windows = [np.arange(i * interval_size, (i + 1) * interval_size) for i in range(N)]\n",
    "    if remainder > 0:\n",
    "        windows[-1] = np.arange((N - 1) * interval_size, total_features)\n",
    "    return windows\n",
    "def divide_data_into_parts(X, num_parts=5):\n",
    "    part_size = X.shape[1] // num_parts\n",
    "    parts = [np.arange(i * part_size, (i + 1) * part_size) for i in range(num_parts)]\n",
    "    return parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_binary_matrix(all_index_subsets, M, weight):\n",
    "    keys = list(all_index_subsets.keys())\n",
    "    n = len(keys)\n",
    "    W = np.column_stack((weight, np.zeros((n, 200))))\n",
    "    binary_matrix = np.zeros((M, n))\n",
    "    for k in range(n):\n",
    "        column = np.concatenate([np.ones(round(weight[k] * M)), \n",
    "                                np.zeros(M - round(weight[k] * M))])\n",
    "        np.random.shuffle(column)  # shuffle the column like randperm\n",
    "        binary_matrix[:, k] = column\n",
    "    return binary_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rmsecv_with_dropped_indices(binary_subset, X, y, kf, frozen_parts):\n",
    "    \"\"\"Compute RMSECV using Partial Least Squares and K-Fold cross-validation.\"\"\"\n",
    "    pls = PLSRegression(n_components=10)\n",
    "    unique_indices = set()\n",
    "    for key, value in binary_subset.items():\n",
    "        unique_indices.update(value)\n",
    "    unique_indices = np.array(sorted(unique_indices))\n",
    "    frozen_parts = frozen_parts.astype(int)\n",
    "    if len(unique_indices) == 0:\n",
    "        scores = -cross_val_score(pls, X[:, frozen_parts], y, cv=kf, scoring='neg_mean_squared_error')\n",
    "        rmsecv = np.sqrt(np.mean(scores))\n",
    "        return rmsecv\n",
    "    combined_indices = np.concatenate([frozen_parts, unique_indices])\n",
    "    combined_indices = combined_indices.astype(int)\n",
    "    scores = -cross_val_score(pls, X[:, combined_indices], y, cv=kf, scoring='neg_mean_squared_error')\n",
    "    rmsecv = np.sqrt(np.mean(scores))\n",
    "    return rmsecv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_sampling_weights(uninfo_indeces, binary_matrix, num_selected_model):\n",
    "    weight = np.sum(binary_matrix[uninfo_indeces, :], axis=0) / num_selected_model\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_search_optimization(binary_subset, X, y, kf, frozen_parts, current_part_indices):\n",
    "    \"\"\"Optimize the width of each selected wavelength interval, ensuring it stays within the current part.\"\"\"    \n",
    "    optimized_subset = binary_subset.copy()  # Start with the initial intervals\n",
    "    for key, interval in binary_subset.items():\n",
    "        left_bound = current_part_indices[0]\n",
    "        right_bound = current_part_indices[-1]\n",
    "        optimized_interval = interval.copy()  # Copy the initial interval\n",
    "        while optimized_interval[0] > left_bound:  # Ensure we don't go beyond the part's left bound\n",
    "            new_interval = np.insert(optimized_interval, 0, optimized_interval[0] - 1)\n",
    "            new_X_subset = np.concatenate([frozen_parts, X[:, new_interval]], axis=1) #############\n",
    "            new_rmsecv = compute_rmsecv_with_dropped_indices(optimized_subset, X, y, kf, frozen_parts)\n",
    "            if new_rmsecv < compute_rmsecv_with_dropped_indices(optimized_subset, X, y, kf, frozen_parts):\n",
    "                optimized_interval = new_interval  # Accept if RMSECV decreases\n",
    "                optimized_subset[key] = optimized_interval\n",
    "            else:\n",
    "                break  # Stop if no improvement\n",
    "        while optimized_interval[-1] < right_bound:  # Ensure we don't go beyond the part's right bound\n",
    "            new_interval = np.append(optimized_interval, optimized_interval[-1] + 1)\n",
    "            new_X_subset = np.concatenate([frozen_parts, X[:, new_interval]], axis=1) ###############\n",
    "            new_rmsecv = compute_rmsecv_with_dropped_indices(optimized_subset, X, y, kf, frozen_parts)\n",
    "            if new_rmsecv < compute_rmsecv_with_dropped_indices(optimized_subset, X, y, kf, frozen_parts):\n",
    "                optimized_interval = new_interval  # Accept if RMSECV decreases\n",
    "                optimized_subset[key] = optimized_interval\n",
    "            else:\n",
    "                break  # Stop if no improvement\n",
    "    return optimized_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_informative_features(X, y, kf, M=500, num_parts=5, intervals_per_part=10, alpha=0.1):\n",
    "    previous_backup = False\n",
    "    rmsecv_min = float('inf')\n",
    "    backup_index_min = {}\n",
    "    parts = divide_data_into_parts(X, num_parts)  # Step 1: Divide features into parts\n",
    "    informative_parts = [None] * num_parts\n",
    "    for part_idx, part_indices in enumerate(parts):\n",
    "        previous_rmsecv_avg = float('inf')\n",
    "        print(f\"Optimizing part {part_idx + 1}/{num_parts}\")\n",
    "        frozen_parts = np.concatenate([informative_parts[i] if informative_parts[i] is not None else parts[i]\n",
    "                            for i in range(num_parts) \n",
    "                            if i != part_idx and (informative_parts[i] is not None or i > part_idx)])\n",
    "        windows = divide_index_part_into_window(parts[part_idx].reshape(1, len(parts[part_idx])), intervals_per_part)\n",
    "        all_index_subsets = {(i, j): windows[j].flatten() for i in range(0, 1) for j in range(len(windows))}\n",
    "        keys = list(all_index_subsets.keys())\n",
    "        keys_flatten = np.arange(0, len(keys))\n",
    "        key_and_flatten_dict = {}\n",
    "        for i in range(len(keys)):\n",
    "            key_and_flatten_dict[i] = keys[i]\n",
    "        weight = np.ones(len(keys)) * 0.5  # Initial weights\n",
    "        iteration = 0\n",
    "        while True:\n",
    "            binary_matrix = weighted_binary_matrix(all_index_subsets, M, weight)\n",
    "            binary_samples = {}\n",
    "            for i in range(M):\n",
    "                binary_subset = {}\n",
    "                selected_flatten_key = keys_flatten[binary_matrix[i] == 1]\n",
    "                for j in range(len(selected_flatten_key)):\n",
    "                    selected_interval = all_index_subsets[key_and_flatten_dict[selected_flatten_key[j]]]\n",
    "                    binary_subset[keys[selected_flatten_key[j]]] = selected_interval\n",
    "                binary_samples[i] = binary_subset\n",
    "            rmsecvs = [compute_rmsecv_with_dropped_indices(binary_subset, X, y, kf, frozen_parts) for binary_subset in binary_samples.values()]\n",
    "            sorted_indices = np.argsort(rmsecvs)[:int(alpha * M)]\n",
    "            rmsecv_avg = np.mean([rmsecvs[i] for i in sorted_indices])\n",
    "            print(f\"Average RMSECV: {rmsecv_avg}\")\n",
    "            if rmsecv_avg >= previous_rmsecv_avg:\n",
    "                print(\"Converged! RMSECV has increased.\")\n",
    "                break\n",
    "            info_subsets = []\n",
    "            info_subsets = [binary_samples[i] for i in sorted_indices]\n",
    "            previous_rmsecv_avg = rmsecv_avg\n",
    "            weight = update_sampling_weights(sorted_indices, binary_matrix, int(alpha * M))\n",
    "            iteration += 1\n",
    "            print(f'iteration: {iteration},        rmsecv_avg: {rmsecv_avg}')\n",
    "        final_subset = info_subsets[np.argmin([rmsecvs[i] for i in sorted_indices])]\n",
    "        if iteration == 0:\n",
    "            final_subset = {}\n",
    "        info_features = []\n",
    "        for k, v in final_subset.items():\n",
    "            info_features.append(list(v))\n",
    "        informative_parts[part_idx] = np.array(info_features).flatten()\n",
    "    print(\"Final informative parts identified.\")\n",
    "    return informative_parts, backup_index_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 1000\n",
    "alpha = .05\n",
    "optimized_intervals, backup_1 = find_informative_features(X, y, kf, M, alpha=alpha, num_parts=3, intervals_per_part=10)\n",
    "print(optimized_intervals)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rpd(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the Ratio of Performance to Deviation (RPD).\n",
    "    RPD is a statistical measure used to evaluate the performance of\n",
    "    quantitative models, especially in spectroscopy and chemometrics.\n",
    "    Args:\n",
    "        y_true (np.ndarray): Array of true (observed) values.\n",
    "        y_pred (np.ndarray): Array of predicted values.\n",
    "    Returns:\n",
    "        float: The calculated RPD value.\n",
    "    Raises:\n",
    "        ValueError: If input arrays are empty or of different lengths.\n",
    "        ZeroDivisionError: If RMSE is zero.\n",
    "    Note:\n",
    "        Higher RPD values indicate better predictive performance.\n",
    "        General guidelines for interpretation:\n",
    "        RPD < 1.5: Poor model\n",
    "        1.5 < RPD < 2.0: Possible to distinguish between high and low values\n",
    "        2.0 < RPD < 2.5: Approximate quantitative predictions\n",
    "        RPD > 2.5: Good to excellent predictions\n",
    "    \"\"\"\n",
    "    if len(y_true) != len(y_pred):\n",
    "        raise ValueError(\"Input arrays must have the same length.\")\n",
    "    if len(y_true) == 0:\n",
    "        raise ValueError(\"Input arrays cannot be empty.\")\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    std_ref = np.std(y_true)\n",
    "    try:\n",
    "        rpd = std_ref / rmse\n",
    "    except ZeroDivisionError:\n",
    "        raise ZeroDivisionError(\"RMSE is zero, RPD cannot be calculated.\")\n",
    "    return rpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet, Lasso, LinearRegression, Ridge\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.svm import SVR\n",
    "mse = []\n",
    "rpd = []\n",
    "r2 = []\n",
    "for i in fold:\n",
    "    pls = PLSRegression(n_components=10, copy=True, scale=True)\n",
    "    X_train, X_test = X[fold[i]['train_index'], :], X[fold[i]['test_index'], :]\n",
    "    y_train, y_test = y[fold[i]['train_index']], y[fold[i]['test_index']]\n",
    "    pls.fit(X_train, y_train)\n",
    "    y_pred = pls.predict(X_test)\n",
    "    mse1 = mean_squared_error(y_true=y_test, y_pred=y_pred)\n",
    "    mse.append(mse1)\n",
    "    rpd1 = calculate_rpd(y_true=y_test, y_pred=y_pred)\n",
    "    rpd.append(rpd1)\n",
    "    r21 = r2_score(y_true=y_test, y_pred=y_pred)\n",
    "    r2.append(r21)\n",
    "print(np.sqrt(np.mean(mse)))\n",
    "print(np.mean(r21))\n",
    "print(np.mean(rpd))\n",
    "pls1 = PLSRegression(n_components=10)\n",
    "pls1.fit(Xcal, ycal)\n",
    "y_pred = pls1.predict(Xtest)\n",
    "y_cal_pred = pls1.predict(Xcal)\n",
    "mse1 = mean_squared_error(y_true=ytest, y_pred=y_pred)\n",
    "mse1_cal = mean_squared_error(y_true=ycal, y_pred=y_cal_pred)\n",
    "r21 = r2_score(y_true=ytest, y_pred=y_pred)\n",
    "rpd1 = calculate_rpd(y_true=ytest, y_pred=y_pred)\n",
    "print(np.sqrt(mse1_cal))\n",
    "print(np.sqrt(mse1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_intervals = [arr for arr in optimized_intervals if arr is not None]\n",
    "final_array = np.concatenate(optimized_intervals)\n",
    "final_array = final_array.astype(int)\n",
    "print(final_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = []\n",
    "r2 = []\n",
    "rpd = []\n",
    "print(f\"r2{r2}\")\n",
    "for i in fold:\n",
    "    pls = PLSRegression(n_components=10, copy=True, scale=True)\n",
    "    X_train, X_test = X[fold[i]['train_index'], :], X[fold[i]['test_index'], :]\n",
    "    y_train, y_test = y[fold[i]['train_index']], y[fold[i]['test_index']]\n",
    "    X_train, X_test = X_train[:, final_array], X_test[:, final_array]\n",
    "    pls.fit(X_train, y_train)\n",
    "    y_pred = pls.predict(X_test)\n",
    "    mse1 = mean_squared_error(y_true=y_test, y_pred=y_pred)\n",
    "    mse.append(mse1)\n",
    "    rpd1 = calculate_rpd(y_true=y_test, y_pred=y_pred)\n",
    "    rpd.append(rpd1)\n",
    "    r21 = r2_score(y_true=y_test, y_pred=y_pred)\n",
    "    r2.append(r21)\n",
    "print(np.sqrt(np.mean(mse)))\n",
    "print(np.mean(r2))\n",
    "print(np.mean(rpd))\n",
    "pls1 = PLSRegression(n_components=10)\n",
    "pls1.fit(Xcal[:, final_array], ycal)\n",
    "y_pred = pls1.predict(Xtest[:, final_array])\n",
    "y_cal_pred = pls1.predict(Xcal[:, final_array])\n",
    "mse1 = mean_squared_error(y_true=ytest, y_pred=y_pred)\n",
    "mse1_cal = mean_squared_error(y_true=ycal, y_pred=y_cal_pred)\n",
    "r21 = r2_score(y_true=ytest, y_pred=y_pred)\n",
    "rpd1 = calculate_rpd(y_true=ytest, y_pred=y_pred)\n",
    "print(np.sqrt(mse1_cal))\n",
    "print(np.sqrt(mse1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "wavelengths = np.arange(850, 2550 + 1, 0.6)\n",
    "selected_wavelengths = 750 + np.array(final_array) * 2\n",
    "counts, bins = np.histogram(selected_wavelengths, bins=np.arange(850, 2552, 0.6))\n",
    "counts *= 50\n",
    "intervals = [(920, 950), (1000, 1100), (1200, 1280), (1400, 1500)]\n",
    "plt.bar(bins[:-1], counts, width=(bins[1] - bins[0]), color='black')\n",
    "for i, (start, end) in enumerate(intervals):\n",
    "    plt.axvspan(start, end, color='gray', alpha=0.5)\n",
    "    plt.text((start + end) / 2, max(counts) * 1.2, str(i + 1), ha='center')\n",
    "plt.xlabel('Wavelength (nm)')\n",
    "plt.ylabel('Count')\n",
    "plt.xlim(850, 2550)\n",
    "plt.ylim(0, 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_intervals_2, backup_2 = find_informative_features(X, y, kf, M, alpha=alpha, num_parts=3)\n",
    "print(optimized_intervals_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_intervals_2 = [arr for arr in optimized_intervals_2 if arr is not None]\n",
    "final_array_2 = np.concatenate(optimized_intervals_2)\n",
    "final_array_2 = final_array_2.astype(int)\n",
    "print(final_array_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "wavelengths = np.arange(850, 2550 + 1, 0.6)\n",
    "selected_wavelengths = 750 + np.array(final_array_2) * 2\n",
    "counts, bins = np.histogram(selected_wavelengths, bins=np.arange(850, 2552, 0.6))\n",
    "counts *= 50\n",
    "intervals = [(920, 950), (1000, 1100), (1200, 1280), (1400, 1500)]\n",
    "plt.bar(bins[:-1], counts, width=(bins[1] - bins[0]), color='black')\n",
    "for i, (start, end) in enumerate(intervals):\n",
    "    plt.axvspan(start, end, color='gray', alpha=0.5)\n",
    "    plt.text((start + end) / 2, max(counts) * 1.2, str(i + 1), ha='center')\n",
    "plt.xlabel('Wavelength (nm)')\n",
    "plt.ylabel('Count')\n",
    "plt.xlim(850, 2550)\n",
    "plt.ylim(0, 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = []\n",
    "r2 = []\n",
    "rpd = []\n",
    "print(f\"r2{r2}\")\n",
    "for i in fold:\n",
    "    pls = PLSRegression(n_components=10, copy=True, scale=True)\n",
    "    X_train, X_test = X[fold[i]['train_index'], :], X[fold[i]['test_index'], :]\n",
    "    y_train, y_test = y[fold[i]['train_index']], y[fold[i]['test_index']]\n",
    "    X_train, X_test = X_train[:, final_array_2], X_test[:, final_array_2]\n",
    "    pls.fit(X_train, y_train)\n",
    "    y_pred = pls.predict(X_test)\n",
    "    mse1 = mean_squared_error(y_true=y_test, y_pred=y_pred)\n",
    "    mse.append(mse1)\n",
    "    rpd1 = calculate_rpd(y_true=y_test, y_pred=y_pred)\n",
    "    rpd.append(rpd1)\n",
    "    r21 = r2_score(y_true=y_test, y_pred=y_pred)\n",
    "    r2.append(r21)\n",
    "print(np.sqrt(np.mean(mse)))\n",
    "print(np.mean(r2))\n",
    "print(np.mean(rpd))\n",
    "pls1 = PLSRegression(n_components=10)\n",
    "pls1.fit(Xcal[:, final_array_2], ycal)\n",
    "y_pred = pls1.predict(Xtest[:, final_array_2])\n",
    "y_cal_pred = pls1.predict(Xcal[:, final_array_2])\n",
    "mse1 = mean_squared_error(y_true=ytest, y_pred=y_pred)\n",
    "mse1_cal = mean_squared_error(y_true=ycal, y_pred=y_cal_pred)\n",
    "r21 = r2_score(y_true=ytest, y_pred=y_pred)\n",
    "rpd1 = calculate_rpd(y_true=ytest, y_pred=y_pred)\n",
    "print(np.sqrt(mse1_cal))\n",
    "print(np.sqrt(mse1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "def transform_X(X, selected_indices_list):\n",
    "    \"\"\"\n",
    "    Transforms the feature matrix X based on the selected indices from multiple arrays.\n",
    "    Args:\n",
    "    X: The feature matrix of shape (samples, features).\n",
    "    selected_indices_list: List of arrays where each array contains the selected indices of features.\n",
    "    Returns:\n",
    "    Transformed feature matrix where features not selected in any array are dropped, \n",
    "    and those selected in one or more arrays are averaged.\n",
    "    \"\"\"\n",
    "    num_features = X.shape[1]\n",
    "    selection_count = np.zeros(num_features, dtype=int)\n",
    "    for selected_indices in selected_indices_list:\n",
    "        mask = np.zeros(num_features, dtype=bool)\n",
    "        mask[selected_indices] = True\n",
    "        selection_count += mask\n",
    "    selected_X = []\n",
    "    for i in range(num_features):\n",
    "        if selection_count[i] > 0:\n",
    "            selected_X.append(X[:, i] / len(selected_indices_list))  # Divide by the number of selections\n",
    "    X_transformed = np.array(selected_X).T\n",
    "    return X_transformed\n",
    "selected_indices_list = [final_array, final_array_2]\n",
    "X_transformed = transform_X(X, selected_indices_list)\n",
    "print(X_transformed.shape)  # To verify the shape after transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = []\n",
    "r2 = []\n",
    "rpd = []\n",
    "print(f\"r2{r2}\")\n",
    "for i in fold:\n",
    "    pls = PLSRegression(n_components=10, copy=True, scale=True)\n",
    "    X_train, X_test = X_transformed[fold[i]['train_index'], :], X_transformed[fold[i]['test_index'], :]\n",
    "    y_train, y_test = y[fold[i]['train_index']], y[fold[i]['test_index']]\n",
    "    pls.fit(X_train, y_train)\n",
    "    y_pred = pls.predict(X_test)\n",
    "    mse1 = mean_squared_error(y_true=y_test, y_pred=y_pred)\n",
    "    mse.append(mse1)\n",
    "    rpd1 = calculate_rpd(y_true=y_test, y_pred=y_pred)\n",
    "    rpd.append(rpd1)\n",
    "    r21 = r2_score(y_true=y_test, y_pred=y_pred)\n",
    "    r2.append(r21)\n",
    "print(np.sqrt(np.mean(mse)))\n",
    "print(np.mean(r2))\n",
    "print(np.mean(rpd))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
