{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration is valid.\n",
      "NUM_FOLDS: 5\n",
      "SHUFFLE_DATA: True\n",
      "RANDOM_SEED: 42\n",
      "CHEMICAL_SUBSTANCES: [5]\n",
      "WINDOW_SIZE_START: 6\n",
      "WINDOW_SIZE_END: 12\n",
      "WINDOW_SIZE_STEP: 2\n",
      "ALPHA_THRESHOLD: 0.05\n",
      "PLS_COMPONENTS: 10\n",
      "dict_keys(['__header__', '__version__', '__globals__', 'VarLabels_X', 'Xcal', 'ycal', 'Xtest', 'ytest'])\n",
      "(415, 100)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import scipy.io\n",
    "from dataset_maker.dataset_maker import create_kfold_splits\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from chemotools.scatter import StandardNormalVariate\n",
    "mat_data = scipy.io.loadmat('./dataset/03/wheat_kernel.mat')\n",
    "print(mat_data.keys())\n",
    "Xtest, Xcal, ytest, ycal = mat_data['Xtest'], mat_data['Xcal'], mat_data['ytest'], mat_data['ycal']\n",
    "print(Xcal.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0:\n",
      "  Train: index=[  1   3   4   5   7   8  12  13  14  16  17  19  20  21  23  24  25  26\n",
      "  27  28  29  31  32  34  35  36  37  38  40  41  42  43  44  45  47  48\n",
      "  49  50  51  52  53  54  56  57  58  59  60  61  62  64  65  66  67  69\n",
      "  71  74  80  81  83  85  86  87  88  91  92  94  95  96  97  98  99 100\n",
      " 102 103 105 106 107 108 109 110 111 112 113 114 115 116 118 119 120 121\n",
      " 122 123 125 126 127 128 129 130 133 134 135 136 138 139 141 142 143 144\n",
      " 145 146 147 149 150 151 152 154 156 157 159 160 161 162 163 164 165 166\n",
      " 168 169 170 171 172 173 174 175 176 178 179 181 183 185 186 187 188 189\n",
      " 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207\n",
      " 208 211 212 213 214 215 216 217 219 220 221 222 223 224 226 227 229 230\n",
      " 231 232 233 235 236 237 238 239 240 241 242 243 244 246 248 249 251 252\n",
      " 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271\n",
      " 272 273 274 275 276 278 279 282 283 284 285 287 288 289 292 293 294 295\n",
      " 296 297 298 300 302 303 304 305 306 308 309 310 311 312 313 314 315 317\n",
      " 318 319 320 321 322 323 325 326 327 328 329 330 332 333 335 337 338 339\n",
      " 340 341 342 343 344 345 346 348 350 351 352 353 354 356 357 358 359 360\n",
      " 361 362 363 364 365 366 367 369 370 371 372 373 376 377 378 379 380 381\n",
      " 382 383 385 386 387 389 390 391 393 394 395 397 398 399 401 402 403 404\n",
      " 405 406 407 408 409 411 412 413 415 416 417 418 419 420 421 422 423 426\n",
      " 427 428 430 431 432 433 435 438 439 440 441 442 443 444 445 446 447 449\n",
      " 450 451 452 453 454 455 456 457 458 459 461 463 464 465 466 467 468 469\n",
      " 470 471 472 474 475 476 478 479 480 481 482 483 484 486 487 488 489 490\n",
      " 491 492 493 494 495 496 497 498 499 500 501 502 503 504 506 507 508 512\n",
      " 514 515 517 520]\n",
      "  Test:  index=[  0   2   6   9  10  11  15  18  22  30  33  39  46  55  63  68  70  72\n",
      "  73  75  76  77  78  79  82  84  89  90  93 101 104 117 124 131 132 137\n",
      " 140 148 153 155 158 167 177 180 182 184 209 210 218 225 228 234 245 247\n",
      " 250 253 277 280 281 286 290 291 299 301 307 316 324 331 334 336 347 349\n",
      " 355 368 374 375 384 388 392 396 400 410 414 424 425 429 434 436 437 448\n",
      " 460 462 473 477 485 505 509 510 511 513 516 518 519 521 522]\n",
      "Fold 1:\n",
      "  Train: index=[  0   1   2   4   6   8   9  10  11  12  13  14  15  18  20  21  22  27\n",
      "  28  30  32  33  34  35  36  38  39  40  41  43  44  46  47  48  49  50\n",
      "  51  52  53  54  55  58  59  61  62  63  64  65  68  70  71  72  73  74\n",
      "  75  76  77  78  79  80  81  82  83  84  85  87  88  89  90  91  92  93\n",
      "  95  96  97  98  99 100 101 102 103 104 105 106 107 109 111 112 115 117\n",
      " 119 120 121 122 123 124 125 127 128 129 130 131 132 133 134 135 136 137\n",
      " 138 139 140 142 143 145 146 147 148 149 150 151 152 153 155 156 158 159\n",
      " 160 161 162 163 164 165 166 167 168 169 170 171 174 177 178 179 180 182\n",
      " 183 184 186 187 188 189 190 191 193 196 197 198 199 200 201 202 205 206\n",
      " 207 209 210 212 213 214 215 216 217 218 219 221 223 224 225 226 228 230\n",
      " 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 250 251\n",
      " 252 253 254 255 256 257 258 259 260 262 263 264 266 267 268 269 270 273\n",
      " 275 276 277 279 280 281 282 283 284 285 286 288 290 291 292 293 294 295\n",
      " 296 297 298 299 300 301 302 303 304 306 307 308 309 310 311 312 313 314\n",
      " 315 316 317 319 320 321 322 324 325 326 327 328 329 330 331 333 334 336\n",
      " 337 339 342 343 344 345 347 349 350 351 353 354 355 356 357 358 359 363\n",
      " 364 366 367 368 370 371 372 374 375 376 377 378 379 380 381 383 384 385\n",
      " 387 388 389 390 391 392 393 395 396 397 399 400 401 402 403 405 406 408\n",
      " 409 410 411 412 413 414 415 416 418 419 421 422 423 424 425 427 428 429\n",
      " 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 447 448\n",
      " 452 454 455 456 457 458 459 460 461 462 463 466 468 469 470 471 473 474\n",
      " 475 476 477 478 479 480 481 482 484 485 488 489 490 491 493 494 496 498\n",
      " 499 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518\n",
      " 519 520 521 522]\n",
      "  Test:  index=[  3   5   7  16  17  19  23  24  25  26  29  31  37  42  45  56  57  60\n",
      "  66  67  69  86  94 108 110 113 114 116 118 126 141 144 154 157 172 173\n",
      " 175 176 181 185 192 194 195 203 204 208 211 220 222 227 229 231 248 249\n",
      " 261 265 271 272 274 278 287 289 305 318 323 332 335 338 340 341 346 348\n",
      " 352 360 361 362 365 369 373 382 386 394 398 404 407 417 420 426 446 449\n",
      " 450 451 453 464 465 467 472 483 486 487 492 495 497 500 501]\n",
      "Fold 2:\n",
      "  Train: index=[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  37  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55\n",
      "  56  57  58  60  61  62  63  64  65  66  67  68  69  70  71  72  73  75\n",
      "  76  77  78  79  80  82  84  85  86  87  88  89  90  91  93  94  95  98\n",
      "  99 100 101 102 104 105 106 107 108 110 113 114 115 116 117 118 120 121\n",
      " 124 126 127 128 130 131 132 133 134 135 136 137 138 140 141 142 144 148\n",
      " 149 151 153 154 155 156 157 158 159 160 161 162 164 166 167 169 170 171\n",
      " 172 173 174 175 176 177 178 180 181 182 184 185 186 187 189 190 191 192\n",
      " 194 195 197 200 201 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 220 221 222 224 225 226 227 228 229 230 231 232 234 235 236\n",
      " 240 241 242 243 245 247 248 249 250 251 252 253 254 256 257 258 259 260\n",
      " 261 263 264 265 267 269 270 271 272 273 274 276 277 278 279 280 281 282\n",
      " 283 285 286 287 288 289 290 291 292 293 295 299 300 301 303 305 306 307\n",
      " 308 309 313 315 316 317 318 319 323 324 325 326 327 328 330 331 332 334\n",
      " 335 336 337 338 339 340 341 343 344 345 346 347 348 349 350 351 352 355\n",
      " 356 357 358 359 360 361 362 363 364 365 366 368 369 372 373 374 375 378\n",
      " 379 380 382 384 385 386 387 388 389 391 392 394 396 397 398 400 401 402\n",
      " 404 406 407 408 409 410 413 414 417 418 419 420 422 423 424 425 426 427\n",
      " 429 430 431 432 434 435 436 437 439 441 442 443 445 446 447 448 449 450\n",
      " 451 452 453 454 456 457 458 459 460 462 463 464 465 466 467 468 470 472\n",
      " 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 491 492\n",
      " 495 496 497 498 500 501 502 505 506 508 509 510 511 513 514 516 517 518\n",
      " 519 520 521 522]\n",
      "  Test:  index=[ 36  38  59  74  81  83  92  96  97 103 109 111 112 119 122 123 125 129\n",
      " 139 143 145 146 147 150 152 163 165 168 179 183 188 193 196 198 199 202\n",
      " 219 223 233 237 238 239 244 246 255 262 266 268 275 284 294 296 297 298\n",
      " 302 304 310 311 312 314 320 321 322 329 333 342 353 354 367 370 371 376\n",
      " 377 381 383 390 393 395 399 403 405 411 412 415 416 421 428 433 438 440\n",
      " 444 455 461 469 471 489 490 493 494 499 503 504 507 512 515]\n",
      "Fold 3:\n",
      "  Train: index=[  0   1   2   3   5   6   7   9  10  11  13  15  16  17  18  19  20  21\n",
      "  22  23  24  25  26  29  30  31  33  34  36  37  38  39  42  43  45  46\n",
      "  48  49  50  52  53  54  55  56  57  58  59  60  63  66  67  68  69  70\n",
      "  71  72  73  74  75  76  77  78  79  80  81  82  83  84  86  87  88  89\n",
      "  90  91  92  93  94  96  97  99 101 102 103 104 105 106 108 109 110 111\n",
      " 112 113 114 116 117 118 119 121 122 123 124 125 126 129 130 131 132 134\n",
      " 137 139 140 141 143 144 145 146 147 148 149 150 151 152 153 154 155 157\n",
      " 158 160 161 163 165 166 167 168 169 172 173 174 175 176 177 179 180 181\n",
      " 182 183 184 185 187 188 189 190 191 192 193 194 195 196 198 199 201 202\n",
      " 203 204 205 207 208 209 210 211 212 214 217 218 219 220 222 223 225 227\n",
      " 228 229 231 233 234 235 237 238 239 241 243 244 245 246 247 248 249 250\n",
      " 251 252 253 255 257 259 261 262 263 264 265 266 268 269 270 271 272 273\n",
      " 274 275 276 277 278 280 281 284 286 287 289 290 291 293 294 295 296 297\n",
      " 298 299 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316\n",
      " 318 319 320 321 322 323 324 328 329 330 331 332 333 334 335 336 338 339\n",
      " 340 341 342 343 344 345 346 347 348 349 350 352 353 354 355 359 360 361\n",
      " 362 363 365 366 367 368 369 370 371 372 373 374 375 376 377 381 382 383\n",
      " 384 385 386 387 388 389 390 392 393 394 395 396 398 399 400 401 403 404\n",
      " 405 407 410 411 412 413 414 415 416 417 419 420 421 424 425 426 427 428\n",
      " 429 430 433 434 435 436 437 438 440 443 444 445 446 448 449 450 451 453\n",
      " 455 458 459 460 461 462 463 464 465 466 467 469 471 472 473 474 475 477\n",
      " 478 479 480 481 483 485 486 487 489 490 491 492 493 494 495 496 497 498\n",
      " 499 500 501 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517\n",
      " 518 519 520 521 522]\n",
      "  Test:  index=[  4   8  12  14  27  28  32  35  40  41  44  47  51  61  62  64  65  85\n",
      "  95  98 100 107 115 120 127 128 133 135 136 138 142 156 159 162 164 170\n",
      " 171 178 186 197 200 206 213 215 216 221 224 226 230 232 236 240 242 254\n",
      " 256 258 260 267 279 282 283 285 288 292 300 317 325 326 327 337 351 356\n",
      " 357 358 364 378 379 380 391 397 402 406 408 409 418 422 423 431 432 439\n",
      " 441 442 447 452 454 456 457 468 470 476 482 484 488 502]\n",
      "Fold 4:\n",
      "  Train: index=[  0   2   3   4   5   6   7   8   9  10  11  12  14  15  16  17  18  19\n",
      "  22  23  24  25  26  27  28  29  30  31  32  33  35  36  37  38  39  40\n",
      "  41  42  44  45  46  47  51  55  56  57  59  60  61  62  63  64  65  66\n",
      "  67  68  69  70  72  73  74  75  76  77  78  79  81  82  83  84  85  86\n",
      "  89  90  92  93  94  95  96  97  98 100 101 103 104 107 108 109 110 111\n",
      " 112 113 114 115 116 117 118 119 120 122 123 124 125 126 127 128 129 131\n",
      " 132 133 135 136 137 138 139 140 141 142 143 144 145 146 147 148 150 152\n",
      " 153 154 155 156 157 158 159 162 163 164 165 167 168 170 171 172 173 175\n",
      " 176 177 178 179 180 181 182 183 184 185 186 188 192 193 194 195 196 197\n",
      " 198 199 200 202 203 204 206 208 209 210 211 213 215 216 218 219 220 221\n",
      " 222 223 224 225 226 227 228 229 230 231 232 233 234 236 237 238 239 240\n",
      " 242 244 245 246 247 248 249 250 253 254 255 256 258 260 261 262 265 266\n",
      " 267 268 271 272 274 275 277 278 279 280 281 282 283 284 285 286 287 288\n",
      " 289 290 291 292 294 296 297 298 299 300 301 302 304 305 307 310 311 312\n",
      " 314 316 317 318 320 321 322 323 324 325 326 327 329 331 332 333 334 335\n",
      " 336 337 338 340 341 342 346 347 348 349 351 352 353 354 355 356 357 358\n",
      " 360 361 362 364 365 367 368 369 370 371 373 374 375 376 377 378 379 380\n",
      " 381 382 383 384 386 388 390 391 392 393 394 395 396 397 398 399 400 402\n",
      " 403 404 405 406 407 408 409 410 411 412 414 415 416 417 418 420 421 422\n",
      " 423 424 425 426 428 429 431 432 433 434 436 437 438 439 440 441 442 444\n",
      " 446 447 448 449 450 451 452 453 454 455 456 457 460 461 462 464 465 467\n",
      " 468 469 470 471 472 473 476 477 482 483 484 485 486 487 488 489 490 492\n",
      " 493 494 495 497 499 500 501 502 503 504 505 507 509 510 511 512 513 515\n",
      " 516 518 519 521 522]\n",
      "  Test:  index=[  1  13  20  21  34  43  48  49  50  52  53  54  58  71  80  87  88  91\n",
      "  99 102 105 106 121 130 134 149 151 160 161 166 169 174 187 189 190 191\n",
      " 201 205 207 212 214 217 235 241 243 251 252 257 259 263 264 269 270 273\n",
      " 276 293 295 303 306 308 309 313 315 319 328 330 339 343 344 345 350 359\n",
      " 363 366 372 385 387 389 401 413 419 427 430 435 443 445 458 459 463 466\n",
      " 474 475 478 479 480 481 491 496 498 506 508 514 517 520]\n"
     ]
    }
   ],
   "source": [
    "X = np.concatenate((Xcal, Xtest))\n",
    "y = np.concatenate((ycal, ytest))\n",
    "fold, kf = create_kfold_splits(input_data=X, target=y, print_info=True)\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mean = np.mean(X, axis=0)\n",
    "X_centered = X - X_mean\n",
    "y_mean = np.mean(y)\n",
    "y_centered = y - y_mean\n",
    "X = X_centered\n",
    "y = y_centered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_index_part_into_window(spectra, N):\n",
    "    \"\"\"Divide spectra into N equal intervals.\"\"\"\n",
    "    interval_size = spectra.shape[1] // N\n",
    "    intervals = [spectra[:, i * interval_size:(i + 1) * interval_size] for i in range(N)]\n",
    "    return intervals\n",
    "def divide_index_spectra_into_window(spectra, N):\n",
    "    total_features = spectra.shape[1]\n",
    "    interval_size = total_features // N\n",
    "    remainder = total_features % N\n",
    "    windows = [np.arange(i * interval_size, (i + 1) * interval_size) for i in range(N)]\n",
    "    if remainder > 0:\n",
    "        windows[-1] = np.arange((N - 1) * interval_size, total_features)\n",
    "    return windows\n",
    "def divide_data_into_parts(X, num_parts=5):\n",
    "    part_size = X.shape[1] // num_parts\n",
    "    parts = [np.arange(i * part_size, (i + 1) * part_size) for i in range(num_parts)]\n",
    "    return parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_binary_matrix(all_index_subsets, M, weight):\n",
    "    keys = list(all_index_subsets.keys())\n",
    "    n = len(keys)\n",
    "    W = np.column_stack((weight, np.zeros((n, 200))))\n",
    "    binary_matrix = np.zeros((M, n))\n",
    "    for k in range(n):\n",
    "        column = np.concatenate([np.ones(round(weight[k] * M)), \n",
    "                                np.zeros(M - round(weight[k] * M))])\n",
    "        np.random.shuffle(column)  # shuffle the column like randperm\n",
    "        binary_matrix[:, k] = column\n",
    "    return binary_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rmsecv_with_dropped_indices(binary_subset, X, y, kf, frozen_parts):\n",
    "    \"\"\"Compute RMSECV using Partial Least Squares and K-Fold cross-validation.\"\"\"\n",
    "    pls = PLSRegression(n_components=10)\n",
    "    unique_indices = set()\n",
    "    for key, value in binary_subset.items():\n",
    "        unique_indices.update(value)\n",
    "    unique_indices = np.array(sorted(unique_indices))\n",
    "    frozen_parts = frozen_parts.astype(int)\n",
    "    if len(unique_indices) == 0:\n",
    "        scores = -cross_val_score(pls, X[:, frozen_parts], y, cv=kf, scoring='neg_mean_squared_error')\n",
    "        rmsecv = np.sqrt(np.mean(scores))\n",
    "        return rmsecv\n",
    "    combined_indices = np.concatenate([frozen_parts, unique_indices])\n",
    "    combined_indices = combined_indices.astype(int)\n",
    "    scores = -cross_val_score(pls, X[:, combined_indices], y, cv=kf, scoring='neg_mean_squared_error')\n",
    "    rmsecv = np.sqrt(np.mean(scores))\n",
    "    return rmsecv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_sampling_weights(uninfo_indeces, binary_matrix, num_selected_model):\n",
    "    weight = np.sum(binary_matrix[uninfo_indeces, :], axis=0) / num_selected_model\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_search_optimization(binary_subset, X, y, kf, frozen_parts, current_part_indices):\n",
    "    \"\"\"Optimize the width of each selected wavelength interval, ensuring it stays within the current part.\"\"\"    \n",
    "    optimized_subset = binary_subset.copy()  # Start with the initial intervals\n",
    "    for key, interval in binary_subset.items():\n",
    "        left_bound = current_part_indices[0]\n",
    "        right_bound = current_part_indices[-1]\n",
    "        optimized_interval = interval.copy()  # Copy the initial interval\n",
    "        while optimized_interval[0] > left_bound:  # Ensure we don't go beyond the part's left bound\n",
    "            new_interval = np.insert(optimized_interval, 0, optimized_interval[0] - 1)\n",
    "            new_X_subset = np.concatenate([frozen_parts, X[:, new_interval]], axis=1) #############\n",
    "            new_rmsecv = compute_rmsecv_with_dropped_indices(optimized_subset, X, y, kf, frozen_parts)\n",
    "            if new_rmsecv < compute_rmsecv_with_dropped_indices(optimized_subset, X, y, kf, frozen_parts):\n",
    "                optimized_interval = new_interval  # Accept if RMSECV decreases\n",
    "                optimized_subset[key] = optimized_interval\n",
    "            else:\n",
    "                break  # Stop if no improvement\n",
    "        while optimized_interval[-1] < right_bound:  # Ensure we don't go beyond the part's right bound\n",
    "            new_interval = np.append(optimized_interval, optimized_interval[-1] + 1)\n",
    "            new_X_subset = np.concatenate([frozen_parts, X[:, new_interval]], axis=1) ###############\n",
    "            new_rmsecv = compute_rmsecv_with_dropped_indices(optimized_subset, X, y, kf, frozen_parts)\n",
    "            if new_rmsecv < compute_rmsecv_with_dropped_indices(optimized_subset, X, y, kf, frozen_parts):\n",
    "                optimized_interval = new_interval  # Accept if RMSECV decreases\n",
    "                optimized_subset[key] = optimized_interval\n",
    "            else:\n",
    "                break  # Stop if no improvement\n",
    "    return optimized_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_informative_features(X, y, kf, M=500, num_parts=5, intervals_per_part=10, alpha=0.1):\n",
    "    previous_backup = False\n",
    "    rmsecv_min = float('inf')\n",
    "    backup_index_min = {}\n",
    "    parts = divide_data_into_parts(X, num_parts)  # Step 1: Divide features into parts\n",
    "    informative_parts = [None] * num_parts\n",
    "    for part_idx, part_indices in enumerate(parts):\n",
    "        previous_rmsecv_avg = float('inf')\n",
    "        print(f\"Optimizing part {part_idx + 1}/{num_parts}\")\n",
    "        frozen_parts = np.concatenate([informative_parts[i] if informative_parts[i] is not None else parts[i]\n",
    "                            for i in range(num_parts) \n",
    "                            if i != part_idx and (informative_parts[i] is not None or i > part_idx)])\n",
    "        windows = divide_index_part_into_window(parts[part_idx].reshape(1, len(parts[part_idx])), intervals_per_part)\n",
    "        all_index_subsets = {(i, j): windows[j].flatten() for i in range(0, 1) for j in range(len(windows))}\n",
    "        keys = list(all_index_subsets.keys())\n",
    "        keys_flatten = np.arange(0, len(keys))\n",
    "        key_and_flatten_dict = {}\n",
    "        for i in range(len(keys)):\n",
    "            key_and_flatten_dict[i] = keys[i]\n",
    "        weight = np.ones(len(keys)) * 0.5  # Initial weights\n",
    "        iteration = 0\n",
    "        while True:\n",
    "            binary_matrix = weighted_binary_matrix(all_index_subsets, M, weight)\n",
    "            binary_samples = {}\n",
    "            for i in range(M):\n",
    "                binary_subset = {}\n",
    "                selected_flatten_key = keys_flatten[binary_matrix[i] == 1]\n",
    "                for j in range(len(selected_flatten_key)):\n",
    "                    selected_interval = all_index_subsets[key_and_flatten_dict[selected_flatten_key[j]]]\n",
    "                    binary_subset[keys[selected_flatten_key[j]]] = selected_interval\n",
    "                binary_samples[i] = binary_subset\n",
    "            rmsecvs = [compute_rmsecv_with_dropped_indices(binary_subset, X, y, kf, frozen_parts) for binary_subset in binary_samples.values()]\n",
    "            sorted_indices = np.argsort(rmsecvs)[:int(alpha * M)]\n",
    "            rmsecv_avg = np.mean([rmsecvs[i] for i in sorted_indices])\n",
    "            print(f\"Average RMSECV: {rmsecv_avg}\")\n",
    "            if rmsecv_avg >= previous_rmsecv_avg:\n",
    "                print(\"Converged! RMSECV has increased.\")\n",
    "                break\n",
    "            info_subsets = []\n",
    "            info_subsets = [binary_samples[i] for i in sorted_indices]\n",
    "            previous_rmsecv_avg = rmsecv_avg\n",
    "            weight = update_sampling_weights(sorted_indices, binary_matrix, int(alpha * M))\n",
    "            iteration += 1\n",
    "            print(f'iteration: {iteration},        rmsecv_avg: {rmsecv_avg}')\n",
    "        final_subset = info_subsets[np.argmin([rmsecvs[i] for i in sorted_indices])]\n",
    "        if iteration == 0:\n",
    "            final_subset = {}\n",
    "        info_features = []\n",
    "        for k, v in final_subset.items():\n",
    "            info_features.append(list(v))\n",
    "        informative_parts[part_idx] = np.array(info_features).flatten()\n",
    "    print(\"Final informative parts identified.\")\n",
    "    return informative_parts, backup_index_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing part 1/3\n",
      "Average RMSECV: 0.5531259104738458\n",
      "iteration: 1,        rmsecv_avg: 0.5531259104738458\n",
      "Average RMSECV: 0.547724873080319\n",
      "iteration: 2,        rmsecv_avg: 0.547724873080319\n",
      "Average RMSECV: 0.547724873080319\n",
      "Converged! RMSECV has increased.\n",
      "Optimizing part 2/3\n",
      "Average RMSECV: 0.5402750561646139\n",
      "iteration: 1,        rmsecv_avg: 0.5402750561646139\n",
      "Average RMSECV: 0.5376372505873465\n",
      "iteration: 2,        rmsecv_avg: 0.5376372505873465\n",
      "Average RMSECV: 0.5372586199380371\n",
      "iteration: 3,        rmsecv_avg: 0.5372586199380371\n",
      "Average RMSECV: 0.5372586199380371\n",
      "Converged! RMSECV has increased.\n",
      "Optimizing part 3/3\n",
      "Average RMSECV: 0.5388620326863334\n",
      "iteration: 1,        rmsecv_avg: 0.5388620326863334\n",
      "Average RMSECV: 0.5348699370019254\n",
      "iteration: 2,        rmsecv_avg: 0.5348699370019254\n",
      "Average RMSECV: 0.5348546327267728\n",
      "iteration: 3,        rmsecv_avg: 0.5348546327267728\n",
      "Average RMSECV: 0.5348546327267728\n",
      "Converged! RMSECV has increased.\n",
      "Final informative parts identified.\n",
      "[array([ 0,  1,  2,  3,  4,  5, 27, 28, 29]), array([33, 34, 35, 42, 43, 44, 51, 52, 53, 54, 55, 56, 60, 61, 62]), array([66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82,\n",
      "       83, 84, 85, 86, 87, 88, 89, 90, 91, 92])]\n"
     ]
    }
   ],
   "source": [
    "M = 500\n",
    "alpha = .05\n",
    "optimized_intervals, backup_1 = find_informative_features(X, y, kf, M, alpha=alpha, num_parts=3, intervals_per_part=10)\n",
    "print(optimized_intervals)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rpd(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the Ratio of Performance to Deviation (RPD).\n",
    "    RPD is a statistical measure used to evaluate the performance of\n",
    "    quantitative models, especially in spectroscopy and chemometrics.\n",
    "    Args:\n",
    "        y_true (np.ndarray): Array of true (observed) values.\n",
    "        y_pred (np.ndarray): Array of predicted values.\n",
    "    Returns:\n",
    "        float: The calculated RPD value.\n",
    "    Raises:\n",
    "        ValueError: If input arrays are empty or of different lengths.\n",
    "        ZeroDivisionError: If RMSE is zero.\n",
    "    Note:\n",
    "        Higher RPD values indicate better predictive performance.\n",
    "        General guidelines for interpretation:\n",
    "        RPD < 1.5: Poor model\n",
    "        1.5 < RPD < 2.0: Possible to distinguish between high and low values\n",
    "        2.0 < RPD < 2.5: Approximate quantitative predictions\n",
    "        RPD > 2.5: Good to excellent predictions\n",
    "    \"\"\"\n",
    "    if len(y_true) != len(y_pred):\n",
    "        raise ValueError(\"Input arrays must have the same length.\")\n",
    "    if len(y_true) == 0:\n",
    "        raise ValueError(\"Input arrays cannot be empty.\")\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    std_ref = np.std(y_true)\n",
    "    try:\n",
    "        rpd = std_ref / rmse\n",
    "    except ZeroDivisionError:\n",
    "        raise ZeroDivisionError(\"RMSE is zero, RPD cannot be calculated.\")\n",
    "    return rpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5596973886133739\n",
      "0.8891501150647292\n",
      "2.8495319598269084\n",
      "0.5410936918630772\n",
      "0.784677404419085\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet, Lasso, LinearRegression, Ridge\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.svm import SVR\n",
    "mse = []\n",
    "rpd = []\n",
    "r2 = []\n",
    "for i in fold:\n",
    "    pls = PLSRegression(n_components=10, copy=True, scale=True)\n",
    "    X_train, X_test = X[fold[i]['train_index'], :], X[fold[i]['test_index'], :]\n",
    "    y_train, y_test = y[fold[i]['train_index']], y[fold[i]['test_index']]\n",
    "    pls.fit(X_train, y_train)\n",
    "    y_pred = pls.predict(X_test)\n",
    "    mse1 = mean_squared_error(y_true=y_test, y_pred=y_pred)\n",
    "    mse.append(mse1)\n",
    "    rpd1 = calculate_rpd(y_true=y_test, y_pred=y_pred)\n",
    "    rpd.append(rpd1)\n",
    "    r21 = r2_score(y_true=y_test, y_pred=y_pred)\n",
    "    r2.append(r21)\n",
    "print(np.sqrt(np.mean(mse)))\n",
    "print(np.mean(r21))\n",
    "print(np.mean(rpd))\n",
    "pls1 = PLSRegression(n_components=10)\n",
    "pls1.fit(Xcal, ycal)\n",
    "y_pred = pls1.predict(Xtest)\n",
    "y_cal_pred = pls1.predict(Xcal)\n",
    "mse1 = mean_squared_error(y_true=ytest, y_pred=y_pred)\n",
    "mse1_cal = mean_squared_error(y_true=ycal, y_pred=y_cal_pred)\n",
    "r21 = r2_score(y_true=ytest, y_pred=y_pred)\n",
    "rpd1 = calculate_rpd(y_true=ytest, y_pred=y_pred)\n",
    "print(np.sqrt(mse1_cal))\n",
    "print(np.sqrt(mse1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5 27 28 29 33 34 35 42 43 44 51 52 53 54 55 56 60 61 62\n",
      " 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89\n",
      " 90 91 92]\n"
     ]
    }
   ],
   "source": [
    "optimized_intervals = [arr for arr in optimized_intervals if arr is not None]\n",
    "final_array = np.concatenate(optimized_intervals)\n",
    "final_array = final_array.astype(int)\n",
    "print(final_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2[]\n",
      "0.5348546327267965\n",
      "0.8838252686826307\n",
      "2.9885791348994326\n",
      "0.5146432255902376\n",
      "0.6998956341501291\n"
     ]
    }
   ],
   "source": [
    "mse = []\n",
    "r2 = []\n",
    "rpd = []\n",
    "print(f\"r2{r2}\")\n",
    "for i in fold:\n",
    "    pls = PLSRegression(n_components=10, copy=True, scale=True)\n",
    "    X_train, X_test = X[fold[i]['train_index'], :], X[fold[i]['test_index'], :]\n",
    "    y_train, y_test = y[fold[i]['train_index']], y[fold[i]['test_index']]\n",
    "    X_train, X_test = X_train[:, final_array], X_test[:, final_array]\n",
    "    pls.fit(X_train, y_train)\n",
    "    y_pred = pls.predict(X_test)\n",
    "    mse1 = mean_squared_error(y_true=y_test, y_pred=y_pred)\n",
    "    mse.append(mse1)\n",
    "    rpd1 = calculate_rpd(y_true=y_test, y_pred=y_pred)\n",
    "    rpd.append(rpd1)\n",
    "    r21 = r2_score(y_true=y_test, y_pred=y_pred)\n",
    "    r2.append(r21)\n",
    "print(np.sqrt(np.mean(mse)))\n",
    "print(np.mean(r2))\n",
    "print(np.mean(rpd))\n",
    "pls1 = PLSRegression(n_components=10)\n",
    "pls1.fit(Xcal[:, final_array], ycal)\n",
    "y_pred = pls1.predict(Xtest[:, final_array])\n",
    "y_cal_pred = pls1.predict(Xcal[:, final_array])\n",
    "mse1 = mean_squared_error(y_true=ytest, y_pred=y_pred)\n",
    "mse1_cal = mean_squared_error(y_true=ycal, y_pred=y_cal_pred)\n",
    "r21 = r2_score(y_true=ytest, y_pred=y_pred)\n",
    "rpd1 = calculate_rpd(y_true=ytest, y_pred=y_pred)\n",
    "print(np.sqrt(mse1_cal))\n",
    "print(np.sqrt(mse1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "wavelengths = np.arange(850, 1050 + 1, 2)\n",
    "selected_wavelengths = 850 + np.array(final_array) * 2\n",
    "\n",
    "# Count occurrences of selected wavelengths (this could be the frequency or binning logic)\n",
    "counts, bins = np.histogram(selected_wavelengths, bins=np.arange(850, 1052, 1))\n",
    "counts *= 50\n",
    "\n",
    "# Define wavelength intervals for shading\n",
    "intervals = [(850, 865), (888, 888), (910, 920), (934, 940), (972, 988), (1012, 1012), (1018, 1020)]\n",
    "\n",
    "plt.bar(bins[:-1], counts, width=(bins[1] - bins[0]), color='black')\n",
    "\n",
    "# Shading the specific intervals (like in the image)\n",
    "for i, (start, end) in enumerate(intervals):\n",
    "    plt.axvspan(start, end, color='gray', alpha=0.5)\n",
    "    plt.text((start + end) / 2, max(counts) * 1.2, str(i + 1), ha='center')\n",
    "\n",
    "# Labels and limits\n",
    "plt.xlabel('Wavelength (nm)')\n",
    "plt.ylabel('Count')\n",
    "plt.xlim(800, 1050)\n",
    "plt.ylim(0, 100)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
